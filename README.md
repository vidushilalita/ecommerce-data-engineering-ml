# RecoMart Data Pipeline

End-to-end data management pipeline for product recommendation system with **real-time streaming capabilities**.

## Overview

This project implements a comprehensive data pipeline for RecoMart's e-commerce recommendation system, covering:
- **Data Ingestion**: Batch (CSV/JSON) and Real-time (Kafka streaming)
- **Data Validation**: Quality checks and profiling
- **Data Preparation**: Cleaning, encoding, normalization
- **Feature Engineering**: User, item, and interaction features
- **Feature Store**: SQLite-based feature management
- **Machine Learning**: Collaborative filtering with SVD
- **Orchestration**: Apache Airflow DAGs
- **Streaming**: Apache Kafka for real-time transactions

## ğŸš€ Quick Start

### 1. Setup Environment

```powershell
# Navigate to project
cd C:\Users\Vidushi.Bisht\Documents\ecommerce-data-engineering-ml

# Create virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Batch Pipeline

```powershell
# Execute complete pipeline
python run_pipeline.py
```

### 3. Run Streaming Pipeline (Optional)

**Start Kafka** (requires separate setup - see `docs/KAFKA_SETUP.md`):

```powershell
# Terminal 1: Start Zookeeper
cd C:\kafka
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

# Terminal 2: Start Kafka
.\bin\windows\kafka-server-start.bat .\config\server.properties

# Terminal 3: Producer
python src/streaming/kafka_producer.py

# Terminal 4: Consumer
python src/streaming/kafka_consumer.py
```

## ğŸ“Š Features

### Batch Processing
- Partitioned data lake storage
- Data quality validation (target â‰¥95%)
- 13 engineered features
- Collaborative filtering model
- Automated orchestration with Airflow

### Real-time Streaming (NEW! ğŸ†•)
- Apache Kafka integration
- Real-time transaction ingestion
- Near real-time feature updates
- Event-driven architecture
- Scalable consumer groups

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch Data  â”‚        â”‚   Kafka      â”‚
â”‚  (CSV/JSON)  â”‚        â”‚   Stream     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â”‚                       â”‚
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Data Lake (Partitioned)      â”‚
â”‚  raw/ | validated/ | prepared/     â”‚
â”‚          features/ | streaming/     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚Feature Storeâ”‚
           â”‚  (SQLite)   â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ ML Model    â”‚
           â”‚   (SVD)     â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚Recommendationsâ”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
ecommerce-data-engineering-ml/
â”œâ”€â”€ data/                      # Raw data sources
â”œâ”€â”€ storage/                   # Data lake (partitioned)
â”‚   â”œâ”€â”€ raw/                  # Batch ingested data
â”‚   â”œâ”€â”€ prepared/             # Cleaned data
â”‚   â”œâ”€â”€ features/             # Engineered features
â”‚   â””â”€â”€ streaming/            # Real-time stream data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/            # Batch data ingestion
â”‚   â”œâ”€â”€ streaming/            # Kafka producer/consumer
â”‚   â”œâ”€â”€ validation/           # Quality checks
â”‚   â”œâ”€â”€ preparation/          # Data cleaning
â”‚   â”œâ”€â”€ features/             # Feature engineering & store
â”‚   â”œâ”€â”€ models/               # ML models
â”‚   â””â”€â”€ utils/                # Common utilities
â”œâ”€â”€ airflow/dags/             # Orchestration DAGs
â”œâ”€â”€ models/                   # Saved models
â”œâ”€â”€ docs/                     # Documentation
â””â”€â”€ run_pipeline.py           # Master pipeline
```

## ğŸ“š Documentation

- [Setup Guide](docs/SETUP_GUIDE.md) - Complete installation and usage
- [Kafka Setup](docs/KAFKA_SETUP.md) - Real-time streaming configuration
- [Airflow Setup](docs/AIRFLOW_SETUP.md) - Orchestration setup
- [Feature Logic](docs/FEATURE_LOGIC.md) - Feature engineering details
- [DVC Workflow](docs/DVC_WORKFLOW.md) - Data versioning
- [Storage Structure](docs/STORAGE_STRUCTURE.md) - Data lake architecture
- [Project Summary](PROJECT_SUMMARY.md) - Complete overview

## ğŸ¯ Success Criteria

- âœ“ Precision@10 â‰¥ 0.15
- âœ“ Recall@10 â‰¥ 0.10
- âœ“ NDCG@10 â‰¥ 0.20
- âœ“ Data Quality Score â‰¥ 95%
- âœ“ Pipeline Latency < 24 hours (batch)
- âœ“ Real-time processing < 1 second (streaming)

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Language | Python 3.8+ |
| Data Processing | pandas, numpy |
| Streaming | Apache Kafka |
| Storage | Parquet, SQLite |
| Feature Store | Custom SQLite |
| ML | scikit-surprise (SVD) |
| Orchestration | Apache Airflow |
| Versioning | DVC |
| Logging | Custom logger |

## ğŸš¦ Running Components

### Batch Pipeline
```powershell
python run_pipeline.py
```

### Individual Stages
```powershell
python src/ingestion/run_all_ingestion.py
python src/validation/validate_data.py
python src/preparation/clean_data.py
python src/features/engineer_features.py
python src/features/feature_store.py
python src/models/collaborative_filtering.py
```

### Streaming
```powershell
python src/streaming/kafka_producer.py
python src/streaming/kafka_consumer.py
```

### Airflow
```powershell
airflow webserver --port 8080  # Terminal 1
airflow scheduler              # Terminal 2
```

## ğŸ“Š Assignment Coverage

âœ… All 10 tasks completed:
1. Problem Formulation
2. Data Ingestion (Batch + Streaming)
3. Raw Data Storage
4. Data Validation
5. Data Preparation
6. Feature Engineering
7. Feature Store
8. Data Versioning (DVC)
9. Model Training & Evaluation
10. Pipeline Orchestration

**Bonus**: Real-time streaming with Apache Kafka!

## ğŸ“ License

Internal project for RecoMart

## ğŸ‘¥ Team

Data Platform Team - RecoMart
