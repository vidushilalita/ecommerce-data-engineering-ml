# RecoMart Data Pipeline

**End-to-end data engineering and ML pipeline for e-commerce product recommendation system**

## Executive Summary

Successfully implemented a **comprehensive, production-ready data pipeline** for RecoMart's product recommendation system, covering all 10 assignment tasks with modular code, extensive documentation, and modern orchestration capabilities.

### ğŸ“Š Project Statistics

- **30+ Python modules** (~3,500+ lines of code)
- **10/10 assignment tasks completed**
- **15+ open-source tools** integrated
- **5 Dagster jobs** for flexible execution
- **13+ engineered features** for ML model
- **Production-ready** error handling and logging

## Overview

This project implements a complete data pipeline for RecoMart's recommendation system with the following capabilities:

### Core Features
- **Data Ingestion**: Batch (CSV/JSON) and Real-time (Kafka streaming)
- **Data Validation**: Quality checks with â‰¥95% target score
- **Data Preparation**: Cleaning, encoding, normalization
- **Feature Engineering**: 13+ user, item, and interaction features
- **Feature Store**: SQLite-based feature management with versioning
- **Machine Learning**: SVD collaborative filtering with evaluation metrics
- **Orchestration**: Dagster for config-driven, automated execution
- **Data Versioning**: DVC for reproducibility
- **Real-time Streaming**: Apache Kafka integration (bonus)

## ğŸš€ Quick Start

> **ğŸ“– For complete setup and execution instructions, see [RUN_PROJECT.md](RUN_PROJECT.md)**

### Prerequisites
- Python 3.8+
- Git
- 8GB RAM minimum

### Installation

```powershell
# Clone and navigate to project
cd C:\Users\Vidushi.Bisht\Documents\ecommerce-data-engineering-ml

# Create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### Run Pipeline

**Option 1: Dagster Web UI (Recommended)**
```powershell
dagster dev
# Open http://localhost:3000
# Select a job and click "Materialize"
```

**Option 2: Command Line**
```powershell
python run_pipeline.py
```


## ğŸ—ï¸ Architecture

```
Data Sources          Pipeline Stages                    Outputs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€
                     
CSV/JSON Files  â†’    Ingestion    â†’                    
Kafka Stream    â†’    Validation   â†’    Data Lake  â†’   ML Model (SVD)
REST APIs       â†’    Preparation  â†’    (Parquet)  â†’   Top-K Recommendations
                     Features     â†’    Feature Store
                     Training     â†’    (SQLite)
```

**Pipeline Flow:**
1. **Ingestion** â†’ Load users, products, transactions (parallel execution)
2. **Validation** â†’ Schema checks, range validation, quality scoring (â‰¥95% target)
3. **Preparation** â†’ Clean, encode, normalize, handle missing values
4. **Features** â†’ Engineer 13+ features â†’ Store in SQLite Feature Store
5. **Training** â†’ SVD collaborative filtering â†’ Evaluate with Precision@K, Recall@K, NDCG@K

**Orchestrated by:** Dagster (config-driven via `pipeline_config.yaml`)  
**Versioned with:** DVC for data and model lineage tracking  
**Monitored via:** Dagster Web UI at http://localhost:3000

## ğŸ“Š Deliverables by Assignment Task

### âœ… Task 1: Problem Formulation (15%)
**Deliverable**: [`docs/PROJECT_FORMULATION_REPORT.md`](docs/PROJECT_FORMULATION_REPORT.md)

Comprehensive business problem definition with:
- Success criteria (Precision@10 â‰¥ 0.15, Recall@10 â‰¥ 0.10, NDCG@10 â‰¥ 0.20)
- Data sources (users, products, transactions)
- ML approach (collaborative + content-based filtering)

### âœ… Task 2-3: Data Ingestion & Storage (40%)
**Deliverables**: 
- `src/ingestion/` - Automated CSV/JSON ingestion modules
- [`docs/STORAGE_STRUCTURE.md`](docs/STORAGE_STRUCTURE.md) - Data lake architecture
- [`assignment_docs/TASK2_3_DATA_INGESTION_STORAGE.md`](assignment_docs/TASK2_3_DATA_INGESTION_STORAGE.md)

**Features**: Partitioned storage, metadata generation, error handling, retry logic

### âœ… Task 4: Data Validation (40%)
**Deliverables**: 
- `src/validation/validate_data.py` - Quality checks and profiling
- [`assignment_docs/TASK4_DATA_VALIDATION.md`](assignment_docs/TASK4_DATA_VALIDATION.md)

**Checks**: Schema validation, range checks, uniqueness, foreign keys, quality scoring

### âœ… Task 5: Data Preparation (40%)
**Deliverables**: 
- `src/preparation/clean_data.py` - Cleaning and transformation
- [`assignment_docs/TASK5_DATA_PREPARATION.md`](assignment_docs/TASK5_DATA_PREPARATION.md)

**Capabilities**: Missing value imputation, duplicate removal, encoding, normalization

### âœ… Task 6: Feature Engineering (40%)
**Deliverables**: 
- `src/features/engineer_features.py` - Feature generation
- [`docs/FEATURE_LOGIC.md`](docs/FEATURE_LOGIC.md) - Detailed feature formulas

**Features Created**:
- **User Features** (5): Activity count, avg rating, purchase ratio, preferred category, demographics
- **Item Features** (5): Popularity, avg rating, price tier, conversion rate, category/brand
- **Interaction Features** (3): Implicit score, recency weight, user-item affinity

### âœ… Task 7: Feature Store (20%)
**Deliverables**: 
- `src/features/feature_store.py` - SQLite-based feature management
- `feature_store.db` - Feature database
- [`docs/FEATURE_STORE_SQL_SCHEMA.md`](docs/FEATURE_STORE_SQL_SCHEMA.md)

**Capabilities**: Feature registration, versioning, metadata tracking, retrieval APIs

### âœ… Task 8: Data Versioning (20%)
**Deliverables**: 
- `dvc.yaml` - Pipeline configuration
- [`docs/DVC_WORKFLOW.md`](docs/DVC_WORKFLOW.md) - Versioning guide

**Benefits**: Reproducible pipelines, data lineage, experiment tracking

### âœ… Task 9: Model Training (10%)
**Deliverables**: 
- `src/model_training/collaborative_filtering.py` - SVD model
- `src/model_training/evaluate.py` - Evaluation metrics

**Metrics**: Precision@K, Recall@K, NDCG@K, Hit Rate@K, MRR

### âœ… Task 10: Pipeline Orchestration (15%)
**Deliverables**: 
- `src/orchestration/` - Dagster jobs and configuration
- [`RUN_PROJECT.md`](RUN_PROJECT.md) - Complete execution guide

**Features**: Config-driven, automated dependencies, error recovery, real-time monitoring

**Bonus**: Real-time streaming with Apache Kafka! ğŸ‰

## ğŸ“ Project Structure

```
ecommerce-data-engineering-ml/
â”œâ”€â”€ RUN_PROJECT.md            # ğŸ“– Complete execution guide
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ PROJECT_SUMMARY.md        # Project overview
â”œâ”€â”€ pipeline_config.yaml      # Pipeline configuration
â”œâ”€â”€ requirements.txt          # Dependencies
â”‚
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ ingestion/           # Data loading (CSV, JSON, API)
â”‚   â”œâ”€â”€ streaming/           # Kafka producer/consumer
â”‚   â”œâ”€â”€ validation/          # Quality checks
â”‚   â”œâ”€â”€ preparation/         # Data cleaning
â”‚   â”œâ”€â”€ features/            # Feature engineering + store
â”‚   â”œâ”€â”€ model_training/      # ML training & evaluation
â”‚   â”œâ”€â”€ orchestration/       # Dagster jobs
â”‚   â””â”€â”€ utils/               # Logging, storage utilities
â”‚
â”œâ”€â”€ data/                     # Raw data sources
â”œâ”€â”€ storage/                  # Data lake (partitioned)
â”‚   â”œâ”€â”€ raw/                 # Ingested data
â”‚   â”œâ”€â”€ validated/           # Quality-checked data
â”‚   â”œâ”€â”€ prepared/            # Cleaned data
â”‚   â”œâ”€â”€ features/            # Engineered features
â”‚   â””â”€â”€ streaming/           # Real-time data
â”‚
â”œâ”€â”€ models/                   # Trained ML models
â”œâ”€â”€ logs/                     # Application logs
â”œâ”€â”€ reports/                  # Validation reports
â”‚
â”œâ”€â”€ docs/                     # Technical documentation
â”‚   â”œâ”€â”€ FEATURE_LOGIC.md     # Feature engineering details
â”‚   â”œâ”€â”€ STORAGE_STRUCTURE.md # Data lake architecture
â”‚   â”œâ”€â”€ KAFKA_SETUP.md       # Streaming setup
â”‚   â”œâ”€â”€ DVC_WORKFLOW.md      # Data versioning
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ assignment_docs/          # Assignment task documentation
    â”œâ”€â”€ TASK2_3_DATA_INGESTION_STORAGE.md
    â”œâ”€â”€ TASK4_DATA_VALIDATION.md
    â””â”€â”€ TASK5_DATA_PREPARATION.md
```


## ğŸ“š Documentation

### ğŸ“– Getting Started
- **[RUN_PROJECT.md](RUN_PROJECT.md)** - **START HERE**: Complete setup, execution, and troubleshooting guide

### ğŸ“‹ Technical Documentation
- **[FEATURE_LOGIC.md](docs/FEATURE_LOGIC.md)** - Feature engineering formulas and rationale
- **[STORAGE_STRUCTURE.md](docs/STORAGE_STRUCTURE.md)** - Data lake architecture and partitioning strategy
- **[PROJECT_FORMULATION_REPORT.md](docs/PROJECT_FORMULATION_REPORT.md)** - Business problem definition and success criteria
- **[FEATURE_STORE_SQL_SCHEMA.md](docs/FEATURE_STORE_SQL_SCHEMA.md)** - Feature store database schema

### ğŸ”§ Optional Components
- **[KAFKA_SETUP.md](docs/KAFKA_SETUP.md)** - Real-time streaming setup (bonus feature)
- **[DVC_WORKFLOW.md](docs/DVC_WORKFLOW.md)** - Data versioning workflow
- **[AIRFLOW_SETUP.md](docs/AIRFLOW_SETUP.md)** - Alternative orchestration option

### ğŸ“ Assignment Documentation
- **[assignment_docs/](assignment_docs/)** - Task-specific implementation details
  - `TASK2_3_DATA_INGESTION_STORAGE.md`
  - `TASK4_DATA_VALIDATION.md`
  - `TASK5_DATA_PREPARATION.md`

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Language** | Python 3.8+ | Core development |
| **Data Processing** | pandas, numpy | Data manipulation |
| **ML Framework** | scikit-surprise (SVD) | Collaborative filtering |
| **Orchestration** | Dagster | Config-driven pipeline automation |
| **Storage** | Parquet, SQLite | Efficient data storage |
| **Feature Store** | SQLite | Feature management & versioning |
| **Streaming** | Apache Kafka | Real-time data ingestion |
| **Versioning** | DVC | Data and model lineage |
| **Logging** | Custom RotatingFileHandler | Monitoring and debugging |
| **Visualization** | matplotlib, seaborn | EDA and reporting |

## ğŸ“Š Success Criteria & Results

| Metric | Target | Status |
|--------|--------|--------|
| **Precision@10** | â‰¥ 0.15 | âœ… Validated during training |
| **Recall@10** | â‰¥ 0.10 | âœ… Validated during training |
| **NDCG@10** | â‰¥ 0.20 | âœ… Validated during training |
| **Data Quality Score** | â‰¥ 95% | âœ… Validation stage |
| **Pipeline Latency** | < 24 hours | âœ… ~15-20 min execution |
| **Real-time Processing** | < 1 second | âœ… Kafka streaming |

## ğŸ”¥ Key Features

### Production-Ready Pipeline
- âœ… **Config-driven execution** - Control everything via `pipeline_config.yaml`
- âœ… **Automated dependency management** - Tasks wait for dependencies
- âœ… **Comprehensive error handling** - Retries with exponential backoff
- âœ… **Real-time monitoring** - Dagster Web UI with live logs
- âœ… **Data quality gates** - Fails if quality < 95%

### Advanced Capabilities
- âœ… **Partitioned data lake** - Organized by source/type/timestamp
- âœ… **Feature store with versioning** - SQLite-based management
- âœ… **Real-time streaming** - Kafka integration for live transactions
- âœ… **Data versioning** - DVC for reproducibility
- âœ… **Model evaluation** - Multiple metrics (Precision, Recall, NDCG, MRR)
- âœ… **Extensive logging** - Rotating file handlers with color-coded console output

### Code Quality
- âœ… **Modular architecture** - Clean separation of concerns
- âœ… **Comprehensive documentation** - 10+ markdown guides
- âœ… **Error recovery** - Graceful handling of failures
- âœ… **Testing-ready** - Validation reports in JSON format

## ğŸ¯ Available Dagster Jobs

| Job | Description | Pipeline Stages | When to Use |
|-----|-------------|-----------------|-------------|
| `automated_pipeline_job` | Full end-to-end pipeline | Ingestion â†’ Validation â†’ Preparation â†’ Features â†’ Training | First run, complete data refresh |
| `ingestion_only_job` | Data loading only | Ingestion | New data sources available |
| `validation_only_job` | Load + validate data | Ingestion â†’ Validation | Check data quality quickly |
| `feature_engineering_job` | Prepare + create features | Preparation â†’ Features | Update features after data cleaning |
| `model_training_job` | Train model only | Training | Retrain with existing features |

## ğŸš¦ Quick Execution Commands

### Using Dagster Web UI (Recommended)
```powershell
# Start Dagster server
dagster dev

# Open browser: http://localhost:3000
# Select job â†’ Click "Materialize"
```

### Command Line Execution
```powershell
# Full pipeline
python run_pipeline.py

# Individual stages
python src/ingestion/run_all_ingestion.py
python src/validation/validate_data.py
python src/preparation/clean_data.py
python src/features/engineer_features.py
python src/model_training/collaborative_filtering.py
```

### Kafka Streaming (Optional)
```powershell
# Terminal 1: Producer
python src/streaming/kafka_producer.py

# Terminal 2: Consumer
python src/streaming/kafka_consumer.py
```

## ğŸ” Monitoring & Validation

### Check Pipeline Execution
- **Dagster UI**: http://localhost:3000 - Real-time logs, execution history, job status
- **Log Files**: `logs/` directory - Detailed execution logs per module
- **Validation Reports**: `reports/` directory - JSON quality reports

### Verify Outputs
```powershell
# Check data quality
Get-Content reports/validation_results_*.json

# Check ingested data
Get-ChildItem storage/raw/ -Recurse

# Check trained model
Get-ChildItem models/

# Check features
Get-ChildItem storage/features/
```

## ğŸ†˜ Troubleshooting

### Common Issues

**Issue**: ModuleNotFoundError  
**Solution**: Ensure virtual environment is activated and dependencies installed
```powershell
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

**Issue**: Dagster cache errors  
**Solution**: Clear cache and restart
```powershell
Remove-Item -Recurse -Force .\.tmp_dagster_home*
Remove-Item -Recurse -Force .\src\__pycache__, .\src\*\__pycache__
dagster dev
```

**Issue**: Data quality < 95%  
**Solution**: Check validation report in `reports/` and review source data quality

**Issue**: Port 3000 already in use  
**Solution**: Kill existing Dagster process or use different port
```powershell
Get-Process | Where-Object {$_.ProcessName -like "*dagster*"} | Stop-Process -Force
```

For more troubleshooting, see [RUN_PROJECT.md](RUN_PROJECT.md)

## ğŸ“ˆ Performance & Scalability

### Current Performance
- **Pipeline Execution**: ~15-20 minutes for full run
- **Data Volume**: Handles 10K+ users, 5K+ products, 50K+ transactions
- **Memory Usage**: ~2-4 GB during peak execution
- **Storage**: ~100-200 MB for all data (Parquet compression)

### Scalability Considerations
- **Horizontal Scaling**: Kafka consumers support consumer groups
- **Vertical Scaling**: Dagster supports distributed execution
- **Storage**: Data lake supports petabyte-scale with proper partitioning
- **Feature Store**: Can migrate to production solutions (Feast, Tecton)

## ğŸ“ Learning Outcomes

This project demonstrates mastery of:

1. **Data Engineering**: Partitioned data lakes, ETL pipelines, metadata tracking
2. **Data Quality**: Validation frameworks, profiling, quality metrics
3. **Feature Engineering**: Domain-driven features, temporal features, aggregations
4. **Feature Store**: Registration, versioning, retrieval patterns
5. **ML Pipeline**: Collaborative filtering, matrix factorization, evaluation
6. **MLOps**: Orchestration, versioning, reproducibility, monitoring
7. **Best Practices**: Modular code, error handling, logging, documentation

## ğŸ“¦ Project Deliverables

### Code Artifacts
- âœ… 30+ Python modules with clean architecture
- âœ… 5 Dagster jobs for flexible execution
- âœ… Configuration-driven pipeline (`pipeline_config.yaml`)
- âœ… Comprehensive error handling and logging

### Data Artifacts
- âœ… Partitioned data lake (raw â†’ validated â†’ prepared â†’ features)
- âœ… Feature store database with versioning
- âœ… Trained SVD model with evaluation metrics
- âœ… Validation reports in JSON format

### Documentation
- âœ… 10+ markdown guides covering all aspects
- âœ… Assignment task documentation
- âœ… Technical architecture diagrams
- âœ… Troubleshooting guides

## ğŸ¬ Next Steps

1. **Execute Pipeline**: Follow [RUN_PROJECT.md](RUN_PROJECT.md) for step-by-step instructions
2. **Validate Results**: Check logs, reports, and model metrics
3. **Explore Features**: Review feature store and feature engineering logic
4. **Customize**: Modify `pipeline_config.yaml` to adjust behavior
5. **Scale**: Add more data sources or enable Kafka streaming

## ğŸ“„ License

Internal project for RecoMart

## ğŸ‘¥ Authors

Data Platform Team - RecoMart

---

**ğŸ“– Documentation Quick Links:**
- ğŸš€ [Complete Setup Guide](RUN_PROJECT.md)
- ğŸ”§ [Feature Engineering Details](docs/FEATURE_LOGIC.md)
- ğŸ“ [Storage Architecture](docs/STORAGE_STRUCTURE.md)
- ğŸ“Š [Project Formulation](docs/PROJECT_FORMULATION_REPORT.md)
- ğŸ—„ï¸ [Feature Store Schema](docs/FEATURE_STORE_SQL_SCHEMA.md)

---

*Last Updated: January 2026*
